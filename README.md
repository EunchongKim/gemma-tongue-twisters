---
license: gemma
language:
- en
base_model:
- google/gemma-2-2b-it
pipeline_tag: text-generation
---

# **Model Demo**

You can try the **Tongue Twister Generator** in the Hugging Face Space](https://huggingface.co/spaces/ec-kim/gemma2-tongue-twister-generator).

## Example Audio Output

Here is a sample audio of a tongue twister generated by the model:

**Prompt**: Generate tongue twisters about key words: seashells

https://github.com/user-attachments/assets/85b425c8-7546-4c8f-a3d2-84843b8fbba8

---

# **Model Card: gemma2-tongue-twister-generator**

## **Model Details**

- **Model Name**: gemma2-tongue-twister-generator
- **Model Type**: Causal Language Model (Fine-tuned with LoRA)
- **Architecture**: Gemma 2 2B (based on Transformer architecture)
- **Base Model**: `google/gemma-2-2b-it`
- **Training Method**: Fine-tuned using Low-Rank Adaptation (LoRA)
- **Languages**: English
- **License**: Gemma License

## **Model Description**

The **gemma2-tongue-twister-generator** is a fine-tuned version of the **Gemma 2 2B** model. It is specifically trained to generate creative and challenging tongue twisters based on given keywords or prompts. The model can create intricate wordplays with rhythmic patterns, making it ideal for educational and entertainment purposes, such as helping individuals practice speech articulation and fluency.

### **Use Cases**

- **Tongue Twister Generation**: Generate custom tongue twisters based on a set of provided keywords.
- **Speech Practice**: Enhance speaking and articulation skills by practicing with challenging wordplay.
- **Entertainment**: Generate engaging and fun tongue twisters for social games or language challenges.

---

## **Basic gemma-2-2b-it Model Inference**

The basic gemma-2-2b-it model can generate tongue twisters, but it has a few limitations. Here's a summary of the observations:

1. It generates tongue twisters with some degree of success.
2. It often generates extra keywords or unrelated text.
3. It doesn't always provide tongue twisters, as in the example with "cinnamon synonym."

### Example Inference Results:

#### 1. Generate tongue twisters about key words: weeping winnie 
**Response**:
Generate tongue twisters about key words: weeping winnie, crying clown.
**Tongue Twister Fun!**  
**Weeping Winnie:**  
* **"Wheezing Willie won't weep; he weeps for Weezy the Whimsical."**  
  (Focus on "Willie", sounds like a guttural 'W') -  
  *Challenge:* Try to say it quickly without making any mistakes while laughing at your own attempts.*
**Crying Clown:**  
  (Incomplete or missing output for the crying clown.)

#### 2. Generate tongue twisters about key words: cinnamon synonym
**Response**:  
*Cinnamon* is a spice that adds warmth and sweetness to dishes.  
- Its flavor profile includes notes of woody, sweet aroma with hints of spicy heat at times!  
Here are some ideas for your cinnamon synonyms list using the above context as inspiration:  
Let me know if you'd like more examples or want help brainstorming other themes around these terms? ðŸ˜Š

### Observations:
- In the first example, the model generated a partial tongue twister but introduced additional and unrelated keywords.
- In the second example, instead of generating a tongue twister, the model provided a descriptive response about cinnamon.

### Limitations:
- The model sometimes generates extra or irrelevant content beyond the requested tongue twisters.
- For certain prompts, like "cinnamon synonym," the model fails to generate a tongue twister and instead offers general or factual information.

---

## **Training Procedure**

- **Dataset**: The model was fine-tuned on a dataset of 1,900 English tongue twisters, where each example contains a prompt (keywords or questions) and an answer (a tongue twister). The dataset is based on examples found in the paper "TongueTwister Games: A New Benchmark and Dataset for Word Play Generation" published at ACL 2023.
  - **Example Prompt**: `"Generate tongue twisters about key words: sea seashell"`
  - **Example Answer**: `"She sells seashells by the seashore."`
  
- **Training Method**: The model was fine-tuned using **LoRA (Low-Rank Adaptation)** without quantization to maintain high precision and model quality.
- **Parameters**:
  - **Learning Rate**: 3e-5
  - **Epochs**: 3
  - **Batch Size**: 8
  - **Gradient Accumulation**: 4 steps
  - **Scheduler**: Cosine Learning Rate Scheduler
  - **Warmup Steps**: 100
  - **Evaluation**: Early stopping was based on validation performance.

## **Model Performance**

- **Metrics**: 
  - **Human Evaluation**: Qualitative evaluation indicates that the model generates coherent and challenging tongue twisters based on keywords.
  - **No Extra Keywords**: The model does not generate keywords not present in the prompt, ensuring output relevance to the given input.

### **Limitations**

- **Rhythmic Patterns**: The model may not perfectly generate complex syllable stress or rhythmic complexity in all tongue twisters, which might affect the challenge level or entertainment factor.
- **Language**: This model is designed to work only with English tongue twisters.

### **Potential Biases**

The model was trained on tongue twisters, which are specific to a subset of the English language and may not capture cultural or linguistic diversity. The dataset could contain patterns reflective of Western language games, which might affect the diversity of the generated text.

---

## **How to Use the Model**

You can use this model for inference using the Hugging Face `transformers` library:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load the model and tokenizer
model_name = "ec-kim/gemma2-tongue-twister-generator"
base_model = AutoModelForCausalLM.from_pretrained("google/gemma-2-2b-it")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, model_name)

# Generate tongue twister
question = "Generate tongue twisters about key words: cinnamon synonym"
inputs = tokenizer(question, return_tensors="pt")

outputs = model.generate(
    inputs.input_ids,
    max_length=100,
    num_return_sequences=1,
    no_repeat_ngram_size=2,
    repetition_penalty=2.0,
    top_k=50,
    top_p=0.95,
    temperature=0.7,
    eos_token_id=tokenizer.eos_token_id,
    do_sample=True
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```
---

## **Sample Output**

After fine-tuning the model with specific tongue twister data, there were notable improvements in the generation of relevant, coherent tongue twisters without the extra keywords often seen in the base model.

### **Fine-Tuned Model Output:**

#### **Prompt**: "Generate tongue twisters about key words: weeping winnie"  
**Output**: "Weeping Winnie wore wimpier wings when she wanted to weep."

#### **Prompt**: "Generate tongue twisters about key words: cinnamon synonymously"  
**Output**: "Cinnamon synonymously sang snell to the sun's sank and so did she when he sat on her, with his scent of a sinfully sweet sandstorm."

### **Comparison Between Models:**

#### **Base Model (gemma-2-2b-it) Output**:

For details on the basic modelâ€™s output, please refer to the [basic model inference results](#basic-gemma2-inference-results).

1. As seen in the base model's response to "weeping winnie," extra, irrelevant keywords were often introduced.

2. The response to "cinnamon synonym" showed that the base model sometimes generated factual content rather than tongue twisters.

#### **Fine-Tuned Model Output**:

1. **Prompt**: "Generate tongue twisters about key words: weeping winnie"  
   **Output**:  
   "Weeping Winnie wore wimpier wings when she wanted to weep."  
   *(Improved coherence with no extra keywords)*

2. **Prompt**: "Generate tongue twisters about key words: cinnamon synonymously"  
   **Output**:  
   "Cinnamon synonymously sang snell to the sun's sank and so did she when he sat on her, with his scent of a sinfully sweet sandstorm."  
   *(Meaningful and creative tongue twister generated)*

### **Improvements Achieved After Fine-Tuning**:

1. **Better Focus on Keywords**:  
   In the base model, irrelevant keywords were often introduced. After fine-tuning, the model demonstrates a better ability to stick to the provided keywords in the prompt without generating unrelated content.

2. **Coherence in Generated Text**:  
   The base model occasionally failed to generate coherent tongue twisters, as shown in the "cinnamon synonym" example. Fine-tuning the model has improved its ability to generate meaningful and coherent tongue twisters for even abstract or difficult prompts.

3. **Creativity in Wordplay**:  
   After fine-tuning, the model demonstrates enhanced creativity in generating playful and complex word combinations, which is crucial for producing entertaining tongue twisters.

### **Limitations of the Fine-Tuned Model**:

While the fine-tuned model has improved in generating coherent tongue twisters:
- **Some prompts might still lead to less creative responses** if the keywords are very abstract.
- **Length control**: The model might sometimes generate longer responses than necessary, though this has been mitigated to some extent through parameter tuning.

---

## **Intended Use**

- **Primary Use Case**: Generating tongue twisters for entertainment, speech practice, or educational purposes.
- **Use with Caution**: Not intended for generating factual or coherent long-form text. The model excels in generating short, playful text but may not produce factual information.

## **Future Work**

- Extend the dataset with more diverse and creative tongue twisters.
- Explore multi-lingual training for generating tongue twisters in other languages.
- Fine-tune on datasets with specific constraints like syllable stress and rhythmic complexity.

## **Contributions**

This model was fine-tuned by using the Hugging Face `transformers` and `peft` libraries. The training dataset is based on the paper ["TongueTwister Games: A New Benchmark and Dataset for Word Play Generation"](https://aclanthology.org/2023.acl-short.51.pdf) from ACL 2023. For any issues, please contact [here](https://github.com/EunchongKim)
